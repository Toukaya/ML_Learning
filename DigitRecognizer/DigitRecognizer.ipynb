{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Import packages",
   "id": "99f4b74f34f73417"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T03:49:18.988703Z",
     "start_time": "2024-04-28T03:49:18.983636Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ],
   "id": "64bb259dbd7274e0",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T03:49:19.056629Z",
     "start_time": "2024-04-28T03:49:19.052949Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision as tv \n",
    "from torchvision.datasets import MNIST\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import Subset"
   ],
   "id": "ab99b72a0d2c7324",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Configurations",
   "id": "645d5695da9faec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T03:49:19.075550Z",
     "start_time": "2024-04-28T03:49:19.070741Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "config = {\n",
    "    \"batch_size\": 32,\n",
    "    \"test_batch_size\": 1000,\n",
    "    \"epochs\": 14,\n",
    "    \"lr\": 1.0,\n",
    "    \"gamma\": 0.7,\n",
    "    \"dry_run\": False,\n",
    "    \"seed\": 1,\n",
    "    \"log_interval\": 10,\n",
    "    \"save_model\": False,\n",
    "    \"data_dir\": \"./data\",\n",
    "    \"subset_size\": 500\n",
    "}"
   ],
   "id": "b5b4a51c39c2f763",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Import and Preprocessing Dataset",
   "id": "531af9eadbdf9c98"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T03:49:19.082320Z",
     "start_time": "2024-04-28T03:49:19.075550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_balanced_subset_indices(targets, num_classes, num_per_class):\n",
    "    indices = []\n",
    "    for i in range(num_classes):\n",
    "        class_indices = (targets == i).nonzero(as_tuple=True)[0]\n",
    "        random_indices = np.random.choice(class_indices, num_per_class, replace=False)\n",
    "        indices.extend(random_indices)\n",
    "    return indices"
   ],
   "id": "fdd0289d0e57d4db",
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T03:49:19.133820Z",
     "start_time": "2024-04-28T03:49:19.086210Z"
    }
   },
   "cell_type": "code",
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "full_train_dataset = MNIST(root = './data', train = True, download = True, transform = transform)\n",
    "test_set = MNIST(root = './data', train = False, download = False, transform = transform)\n",
    "\n",
    "num_classes = 10\n",
    "num_per_class = 50\n",
    "\n",
    "train_indices = get_balanced_subset_indices(full_train_dataset.targets, num_classes, num_per_class)\n",
    "\n",
    "train_set = Subset(full_train_dataset, train_indices) if config['subset_size'] is None else full_train_dataset"
   ],
   "id": "dbd0422d1a9e2fe2",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T03:49:19.139094Z",
     "start_time": "2024-04-28T03:49:19.135831Z"
    }
   },
   "cell_type": "code",
   "source": "print(train_indices)",
   "id": "e7920a2142a13c46",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46896, 51980, 47389, 46740, 22208, 45235, 23210, 39856, 28971, 30434, 19504, 29052, 15711, 19303, 51892, 4756, 2493, 19292, 58952, 7465, 55917, 29560, 48196, 46871, 25650, 48322, 25278, 24346, 37079, 24661, 45511, 42157, 46627, 13880, 49639, 34580, 19490, 55006, 21635, 24388, 43728, 4002, 10471, 54595, 53893, 2411, 26040, 42552, 31079, 26713, 16718, 4160, 32054, 26125, 12226, 25436, 34272, 32795, 58887, 48361, 25853, 41282, 4066, 57599, 34010, 11236, 27501, 25751, 39776, 40004, 20699, 4972, 26827, 11352, 40696, 4129, 16233, 42874, 36145, 9069, 14310, 397, 40551, 43118, 16525, 44650, 32353, 24626, 1132, 43872, 55609, 48741, 33110, 4186, 29062, 14992, 59114, 49592, 5245, 10741, 44639, 48017, 10582, 7266, 32627, 46867, 5135, 45022, 15363, 23578, 46574, 11109, 15712, 54330, 42789, 14517, 52839, 14458, 51458, 33775, 50913, 52083, 16018, 19115, 48220, 1754, 18815, 30997, 38692, 42439, 43376, 50271, 4380, 24035, 28335, 1920, 53541, 44993, 16527, 59621, 44347, 27525, 1188, 36485, 2171, 18633, 46317, 26477, 9249, 16456, 46918, 45008, 26975, 17745, 51717, 8455, 32730, 57872, 17292, 629, 38174, 46241, 48324, 43230, 3815, 36130, 59178, 45239, 35805, 10827, 31636, 37355, 51613, 23076, 17553, 15453, 16873, 21986, 14825, 52487, 59408, 18099, 21018, 7634, 25210, 51909, 20587, 46803, 34289, 14691, 45408, 35143, 43783, 50658, 18049, 7625, 21153, 32665, 35422, 24299, 35166, 2424, 42116, 33320, 8887, 26634, 5122, 13385, 2451, 32911, 11209, 54158, 4297, 24822, 25401, 36577, 24334, 51652, 42346, 10960, 17730, 54588, 28493, 39209, 12306, 45100, 6536, 18473, 39812, 15891, 47680, 6941, 3092, 14351, 20276, 40030, 15283, 53739, 38984, 48853, 50005, 51182, 35583, 23274, 17493, 45172, 4410, 15376, 43073, 16851, 45432, 33001, 42517, 58808, 2639, 50533, 37089, 51775, 42652, 47741, 39958, 2946, 18347, 51727, 31489, 56982, 39015, 43221, 27375, 6086, 19083, 47919, 22380, 16533, 1135, 47856, 27777, 21850, 25508, 56669, 3129, 22310, 8569, 31186, 31891, 56185, 40966, 15285, 49611, 24217, 51124, 44907, 13360, 56371, 31069, 52131, 20359, 45011, 335, 14591, 59841, 4251, 34255, 44754, 45910, 28163, 11941, 55066, 52412, 28807, 50817, 40707, 21764, 39567, 27559, 2096, 46218, 41577, 50811, 50506, 37676, 42166, 59065, 10774, 12855, 59073, 37346, 55900, 42066, 40394, 23233, 52141, 56421, 12262, 55975, 13595, 43892, 33584, 26181, 23577, 14155, 13167, 52620, 59801, 35184, 32569, 2486, 46109, 18221, 22323, 58624, 33867, 12956, 29338, 53302, 22498, 55689, 44015, 5912, 36603, 57386, 59739, 18402, 37853, 58778, 35499, 48501, 48519, 59560, 23902, 10181, 13637, 57110, 23442, 53612, 30670, 11220, 30339, 13867, 50021, 42593, 20318, 16882, 14891, 11660, 17383, 55444, 26589, 50325, 55446, 40512, 26964, 5352, 12123, 45059, 38282, 25319, 14379, 59925, 6590, 59751, 47462, 10726, 47989, 54919, 30999, 45313, 9241, 19267, 45896, 44875, 37891, 45317, 18577, 18335, 15258, 1016, 57352, 41550, 45746, 792, 23964, 51013, 28779, 30450, 12778, 24791, 4609, 25455, 28480, 15879, 55221, 43097, 40437, 43403, 14474, 42661, 12130, 10584, 14697, 23035, 6760, 38596, 14908, 36891, 11546, 16994, 24760, 41560, 57640, 21267, 54038, 11628, 27295, 15239, 20103, 45925, 45065, 6073, 34420, 38552, 19513, 51123, 3653, 6301, 58796, 44228, 46087, 7239, 43596, 10752, 25280, 6376, 58616, 54538, 50659, 18082, 16147, 36906, 9128, 51185, 13108, 16106, 40439, 20050, 6589, 49511, 25624, 18917, 38006, 53544, 45980, 29400, 36481, 38690, 59761, 12868, 7061, 56092, 12162]\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Nerual Network Definition",
   "id": "9b0501fba37c3359"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T03:49:19.146881Z",
     "start_time": "2024-04-28T03:49:19.139600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.25)\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(9216, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc_layers(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ],
   "id": "b87dbd076dd1556f",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluate the model",
   "id": "526d582b43ba81ed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T03:49:19.154458Z",
     "start_time": "2024-04-28T03:49:19.146881Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args['log_interval'] == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if args['dry_run']:\n",
    "                break"
   ],
   "id": "e2b927698f1bf9cf",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T03:49:19.162299Z",
     "start_time": "2024-04-28T03:49:19.156519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ],
   "id": "e6cc0322ae0914a3",
   "outputs": [],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T03:54:04.251582Z",
     "start_time": "2024-04-28T03:54:04.151922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_kwargs = {'batch_size': config['batch_size']}\n",
    "test_kwargs = {'batch_size': config['test_batch_size']}\n",
    "if torch.cuda.is_available():\n",
    "    cuda_kwargs = {'num_workers': 1,\n",
    "                   'pin_memory': True,\n",
    "                   'shuffle': True}\n",
    "    train_kwargs.update(cuda_kwargs)\n",
    "    test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "for i in range(len(train_set)):\n",
    "    data, target = train_set[i]\n",
    "    print(f'{i}:{target}')\n",
    "train_loader = DataLoader(train_set, **train_kwargs)\n",
    "test_loader = DataLoader(test_set, **test_kwargs)"
   ],
   "id": "5cb3c587bf5db324",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:0\n",
      "1:0\n",
      "2:0\n",
      "3:0\n",
      "4:0\n",
      "5:0\n",
      "6:0\n",
      "7:0\n",
      "8:0\n",
      "9:0\n",
      "10:0\n",
      "11:0\n",
      "12:0\n",
      "13:0\n",
      "14:0\n",
      "15:0\n",
      "16:0\n",
      "17:0\n",
      "18:0\n",
      "19:0\n",
      "20:0\n",
      "21:0\n",
      "22:0\n",
      "23:0\n",
      "24:0\n",
      "25:0\n",
      "26:0\n",
      "27:0\n",
      "28:0\n",
      "29:0\n",
      "30:0\n",
      "31:0\n",
      "32:0\n",
      "33:0\n",
      "34:0\n",
      "35:0\n",
      "36:0\n",
      "37:0\n",
      "38:0\n",
      "39:0\n",
      "40:0\n",
      "41:0\n",
      "42:0\n",
      "43:0\n",
      "44:0\n",
      "45:0\n",
      "46:0\n",
      "47:0\n",
      "48:0\n",
      "49:0\n",
      "50:1\n",
      "51:1\n",
      "52:1\n",
      "53:1\n",
      "54:1\n",
      "55:1\n",
      "56:1\n",
      "57:1\n",
      "58:1\n",
      "59:1\n",
      "60:1\n",
      "61:1\n",
      "62:1\n",
      "63:1\n",
      "64:1\n",
      "65:1\n",
      "66:1\n",
      "67:1\n",
      "68:1\n",
      "69:1\n",
      "70:1\n",
      "71:1\n",
      "72:1\n",
      "73:1\n",
      "74:1\n",
      "75:1\n",
      "76:1\n",
      "77:1\n",
      "78:1\n",
      "79:1\n",
      "80:1\n",
      "81:1\n",
      "82:1\n",
      "83:1\n",
      "84:1\n",
      "85:1\n",
      "86:1\n",
      "87:1\n",
      "88:1\n",
      "89:1\n",
      "90:1\n",
      "91:1\n",
      "92:1\n",
      "93:1\n",
      "94:1\n",
      "95:1\n",
      "96:1\n",
      "97:1\n",
      "98:1\n",
      "99:1\n",
      "100:2\n",
      "101:2\n",
      "102:2\n",
      "103:2\n",
      "104:2\n",
      "105:2\n",
      "106:2\n",
      "107:2\n",
      "108:2\n",
      "109:2\n",
      "110:2\n",
      "111:2\n",
      "112:2\n",
      "113:2\n",
      "114:2\n",
      "115:2\n",
      "116:2\n",
      "117:2\n",
      "118:2\n",
      "119:2\n",
      "120:2\n",
      "121:2\n",
      "122:2\n",
      "123:2\n",
      "124:2\n",
      "125:2\n",
      "126:2\n",
      "127:2\n",
      "128:2\n",
      "129:2\n",
      "130:2\n",
      "131:2\n",
      "132:2\n",
      "133:2\n",
      "134:2\n",
      "135:2\n",
      "136:2\n",
      "137:2\n",
      "138:2\n",
      "139:2\n",
      "140:2\n",
      "141:2\n",
      "142:2\n",
      "143:2\n",
      "144:2\n",
      "145:2\n",
      "146:2\n",
      "147:2\n",
      "148:2\n",
      "149:2\n",
      "150:3\n",
      "151:3\n",
      "152:3\n",
      "153:3\n",
      "154:3\n",
      "155:3\n",
      "156:3\n",
      "157:3\n",
      "158:3\n",
      "159:3\n",
      "160:3\n",
      "161:3\n",
      "162:3\n",
      "163:3\n",
      "164:3\n",
      "165:3\n",
      "166:3\n",
      "167:3\n",
      "168:3\n",
      "169:3\n",
      "170:3\n",
      "171:3\n",
      "172:3\n",
      "173:3\n",
      "174:3\n",
      "175:3\n",
      "176:3\n",
      "177:3\n",
      "178:3\n",
      "179:3\n",
      "180:3\n",
      "181:3\n",
      "182:3\n",
      "183:3\n",
      "184:3\n",
      "185:3\n",
      "186:3\n",
      "187:3\n",
      "188:3\n",
      "189:3\n",
      "190:3\n",
      "191:3\n",
      "192:3\n",
      "193:3\n",
      "194:3\n",
      "195:3\n",
      "196:3\n",
      "197:3\n",
      "198:3\n",
      "199:3\n",
      "200:4\n",
      "201:4\n",
      "202:4\n",
      "203:4\n",
      "204:4\n",
      "205:4\n",
      "206:4\n",
      "207:4\n",
      "208:4\n",
      "209:4\n",
      "210:4\n",
      "211:4\n",
      "212:4\n",
      "213:4\n",
      "214:4\n",
      "215:4\n",
      "216:4\n",
      "217:4\n",
      "218:4\n",
      "219:4\n",
      "220:4\n",
      "221:4\n",
      "222:4\n",
      "223:4\n",
      "224:4\n",
      "225:4\n",
      "226:4\n",
      "227:4\n",
      "228:4\n",
      "229:4\n",
      "230:4\n",
      "231:4\n",
      "232:4\n",
      "233:4\n",
      "234:4\n",
      "235:4\n",
      "236:4\n",
      "237:4\n",
      "238:4\n",
      "239:4\n",
      "240:4\n",
      "241:4\n",
      "242:4\n",
      "243:4\n",
      "244:4\n",
      "245:4\n",
      "246:4\n",
      "247:4\n",
      "248:4\n",
      "249:4\n",
      "250:5\n",
      "251:5\n",
      "252:5\n",
      "253:5\n",
      "254:5\n",
      "255:5\n",
      "256:5\n",
      "257:5\n",
      "258:5\n",
      "259:5\n",
      "260:5\n",
      "261:5\n",
      "262:5\n",
      "263:5\n",
      "264:5\n",
      "265:5\n",
      "266:5\n",
      "267:5\n",
      "268:5\n",
      "269:5\n",
      "270:5\n",
      "271:5\n",
      "272:5\n",
      "273:5\n",
      "274:5\n",
      "275:5\n",
      "276:5\n",
      "277:5\n",
      "278:5\n",
      "279:5\n",
      "280:5\n",
      "281:5\n",
      "282:5\n",
      "283:5\n",
      "284:5\n",
      "285:5\n",
      "286:5\n",
      "287:5\n",
      "288:5\n",
      "289:5\n",
      "290:5\n",
      "291:5\n",
      "292:5\n",
      "293:5\n",
      "294:5\n",
      "295:5\n",
      "296:5\n",
      "297:5\n",
      "298:5\n",
      "299:5\n",
      "300:6\n",
      "301:6\n",
      "302:6\n",
      "303:6\n",
      "304:6\n",
      "305:6\n",
      "306:6\n",
      "307:6\n",
      "308:6\n",
      "309:6\n",
      "310:6\n",
      "311:6\n",
      "312:6\n",
      "313:6\n",
      "314:6\n",
      "315:6\n",
      "316:6\n",
      "317:6\n",
      "318:6\n",
      "319:6\n",
      "320:6\n",
      "321:6\n",
      "322:6\n",
      "323:6\n",
      "324:6\n",
      "325:6\n",
      "326:6\n",
      "327:6\n",
      "328:6\n",
      "329:6\n",
      "330:6\n",
      "331:6\n",
      "332:6\n",
      "333:6\n",
      "334:6\n",
      "335:6\n",
      "336:6\n",
      "337:6\n",
      "338:6\n",
      "339:6\n",
      "340:6\n",
      "341:6\n",
      "342:6\n",
      "343:6\n",
      "344:6\n",
      "345:6\n",
      "346:6\n",
      "347:6\n",
      "348:6\n",
      "349:6\n",
      "350:7\n",
      "351:7\n",
      "352:7\n",
      "353:7\n",
      "354:7\n",
      "355:7\n",
      "356:7\n",
      "357:7\n",
      "358:7\n",
      "359:7\n",
      "360:7\n",
      "361:7\n",
      "362:7\n",
      "363:7\n",
      "364:7\n",
      "365:7\n",
      "366:7\n",
      "367:7\n",
      "368:7\n",
      "369:7\n",
      "370:7\n",
      "371:7\n",
      "372:7\n",
      "373:7\n",
      "374:7\n",
      "375:7\n",
      "376:7\n",
      "377:7\n",
      "378:7\n",
      "379:7\n",
      "380:7\n",
      "381:7\n",
      "382:7\n",
      "383:7\n",
      "384:7\n",
      "385:7\n",
      "386:7\n",
      "387:7\n",
      "388:7\n",
      "389:7\n",
      "390:7\n",
      "391:7\n",
      "392:7\n",
      "393:7\n",
      "394:7\n",
      "395:7\n",
      "396:7\n",
      "397:7\n",
      "398:7\n",
      "399:7\n",
      "400:8\n",
      "401:8\n",
      "402:8\n",
      "403:8\n",
      "404:8\n",
      "405:8\n",
      "406:8\n",
      "407:8\n",
      "408:8\n",
      "409:8\n",
      "410:8\n",
      "411:8\n",
      "412:8\n",
      "413:8\n",
      "414:8\n",
      "415:8\n",
      "416:8\n",
      "417:8\n",
      "418:8\n",
      "419:8\n",
      "420:8\n",
      "421:8\n",
      "422:8\n",
      "423:8\n",
      "424:8\n",
      "425:8\n",
      "426:8\n",
      "427:8\n",
      "428:8\n",
      "429:8\n",
      "430:8\n",
      "431:8\n",
      "432:8\n",
      "433:8\n",
      "434:8\n",
      "435:8\n",
      "436:8\n",
      "437:8\n",
      "438:8\n",
      "439:8\n",
      "440:8\n",
      "441:8\n",
      "442:8\n",
      "443:8\n",
      "444:8\n",
      "445:8\n",
      "446:8\n",
      "447:8\n",
      "448:8\n",
      "449:8\n",
      "450:9\n",
      "451:9\n",
      "452:9\n",
      "453:9\n",
      "454:9\n",
      "455:9\n",
      "456:9\n",
      "457:9\n",
      "458:9\n",
      "459:9\n",
      "460:9\n",
      "461:9\n",
      "462:9\n",
      "463:9\n",
      "464:9\n",
      "465:9\n",
      "466:9\n",
      "467:9\n",
      "468:9\n",
      "469:9\n",
      "470:9\n",
      "471:9\n",
      "472:9\n",
      "473:9\n",
      "474:9\n",
      "475:9\n",
      "476:9\n",
      "477:9\n",
      "478:9\n",
      "479:9\n",
      "480:9\n",
      "481:9\n",
      "482:9\n",
      "483:9\n",
      "484:9\n",
      "485:9\n",
      "486:9\n",
      "487:9\n",
      "488:9\n",
      "489:9\n",
      "490:9\n",
      "491:9\n",
      "492:9\n",
      "493:9\n",
      "494:9\n",
      "495:9\n",
      "496:9\n",
      "497:9\n",
      "498:9\n",
      "499:9\n"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T03:50:35.569953Z",
     "start_time": "2024-04-28T03:49:19.191414Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = Net().to(device)\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=config['lr'])\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=config['gamma'])\n",
    "for epoch in range(1, config['epochs'] + 1):\n",
    "    train(config, model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)\n",
    "    scheduler.step()\n",
    "\n",
    "if config['save_model']:\n",
    "    torch.save(model.state_dict(), \"mnist_cnn.pt\")"
   ],
   "id": "f5174b840cc94057",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/500 (0%)]\tLoss: 2.288975\n",
      "Train Epoch: 1 [320/500 (62%)]\tLoss: 2.297476\n",
      "\n",
      "Test set: Average loss: 2.2987, Accuracy: 1614/10000 (16%)\n",
      "\n",
      "Train Epoch: 2 [0/500 (0%)]\tLoss: 2.251512\n",
      "Train Epoch: 2 [320/500 (62%)]\tLoss: 2.276959\n",
      "\n",
      "Test set: Average loss: 2.2787, Accuracy: 1829/10000 (18%)\n",
      "\n",
      "Train Epoch: 3 [0/500 (0%)]\tLoss: 2.147706\n",
      "Train Epoch: 3 [320/500 (62%)]\tLoss: 2.198101\n",
      "\n",
      "Test set: Average loss: 2.1761, Accuracy: 2891/10000 (29%)\n",
      "\n",
      "Train Epoch: 4 [0/500 (0%)]\tLoss: 1.631658\n",
      "Train Epoch: 4 [320/500 (62%)]\tLoss: 1.884598\n",
      "\n",
      "Test set: Average loss: 1.9264, Accuracy: 4421/10000 (44%)\n",
      "\n",
      "Train Epoch: 5 [0/500 (0%)]\tLoss: 1.300213\n",
      "Train Epoch: 5 [320/500 (62%)]\tLoss: 1.414535\n",
      "\n",
      "Test set: Average loss: 1.6092, Accuracy: 5993/10000 (60%)\n",
      "\n",
      "Train Epoch: 6 [0/500 (0%)]\tLoss: 0.866878\n",
      "Train Epoch: 6 [320/500 (62%)]\tLoss: 1.021146\n",
      "\n",
      "Test set: Average loss: 1.2803, Accuracy: 6673/10000 (67%)\n",
      "\n",
      "Train Epoch: 7 [0/500 (0%)]\tLoss: 0.701529\n",
      "Train Epoch: 7 [320/500 (62%)]\tLoss: 0.729161\n",
      "\n",
      "Test set: Average loss: 1.0566, Accuracy: 7353/10000 (74%)\n",
      "\n",
      "Train Epoch: 8 [0/500 (0%)]\tLoss: 0.633699\n",
      "Train Epoch: 8 [320/500 (62%)]\tLoss: 0.690400\n",
      "\n",
      "Test set: Average loss: 0.9064, Accuracy: 7733/10000 (77%)\n",
      "\n",
      "Train Epoch: 9 [0/500 (0%)]\tLoss: 0.525935\n",
      "Train Epoch: 9 [320/500 (62%)]\tLoss: 0.621666\n",
      "\n",
      "Test set: Average loss: 0.8180, Accuracy: 7998/10000 (80%)\n",
      "\n",
      "Train Epoch: 10 [0/500 (0%)]\tLoss: 0.337264\n",
      "Train Epoch: 10 [320/500 (62%)]\tLoss: 0.616629\n",
      "\n",
      "Test set: Average loss: 0.7553, Accuracy: 8173/10000 (82%)\n",
      "\n",
      "Train Epoch: 11 [0/500 (0%)]\tLoss: 0.411717\n",
      "Train Epoch: 11 [320/500 (62%)]\tLoss: 0.445532\n",
      "\n",
      "Test set: Average loss: 0.7194, Accuracy: 8284/10000 (83%)\n",
      "\n",
      "Train Epoch: 12 [0/500 (0%)]\tLoss: 0.427356\n",
      "Train Epoch: 12 [320/500 (62%)]\tLoss: 0.557126\n",
      "\n",
      "Test set: Average loss: 0.6947, Accuracy: 8352/10000 (84%)\n",
      "\n",
      "Train Epoch: 13 [0/500 (0%)]\tLoss: 0.347085\n",
      "Train Epoch: 13 [320/500 (62%)]\tLoss: 0.728520\n",
      "\n",
      "Test set: Average loss: 0.6797, Accuracy: 8392/10000 (84%)\n",
      "\n",
      "Train Epoch: 14 [0/500 (0%)]\tLoss: 0.432535\n",
      "Train Epoch: 14 [320/500 (62%)]\tLoss: 0.535203\n",
      "\n",
      "Test set: Average loss: 0.6697, Accuracy: 8411/10000 (84%)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 77
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
